{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjxlw63Ylg5HT5k3pHwPHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahkaarina/lazy-language/blob/main/transcribing_data/demo_transcribe_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcribing data\n",
        "\n",
        "**This basic notebook will talk you through how to transcribe pre-recorded audio using the Whisper AI**\n",
        "\n",
        "For more information on Whisper and other tutorials (and where I got most of the information myself on how to use it), see the reference documentation at the bottom of this notebook."
      ],
      "metadata": {
        "id": "e4Ow4EG7_fKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**\n",
        "\n",
        "Whisper is not included in Collab, so let's install it using pip.\n",
        "\n",
        "Everytime you re-set (?) this notebook, collab will ask you to reinstall."
      ],
      "metadata": {
        "id": "RMb-x7Ck_zgX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "qH1T-whl_bm9",
        "outputId": "fe940fdf-8b8b-4439-f711-18275ac27c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803321 sha256=9b3563799558d4a42434912d9dc943293990ffe0211bfb81ce9890cb91d13181\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whisper"
                ]
              },
              "id": "ab57d1264c0245a4aedfadb7b826fe61"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**\n",
        "\n",
        "Load in the relevant libraries (including our now newly installed whisper)."
      ],
      "metadata": {
        "id": "9Zmb9OGDAfGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library we will use to define and read in our file paths:\n",
        "from pathlib import Path\n",
        "\n",
        "# Library to read and manage json files:\n",
        "import json\n",
        "\n",
        "# Library to transcibe our data:\n",
        "import whisper\n",
        "\n",
        "# And the usual suspects, cioè libraries we need to wrangle and organizie data:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Display images:\n",
        "from PIL import Image\n",
        "from IPython.display import display\n"
      ],
      "metadata": {
        "id": "1DiZh9NUAk1K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**\n",
        "\n",
        "Load the relevant whisper model.\n",
        "\n",
        "Whisper has 6 different model, which range in size (and therefore speed; the smaller the model, the faster the transcription).\n",
        "\n",
        "For those interested in further details on Whisper itself, the source documentation can be found here: https://github.com/openai/whisper\n",
        "\n",
        "In brief, whisper is a speech-recognition model that has been trained on large number of both English and multilingual data. Whisper can transcibe, translate and identify the language used in your speech data.\n",
        "\n",
        "The screenshot below (taken from the Whisper landing page) shows the 6 different models. Let's breakdown their differences.\n",
        "\n",
        "\n",
        "*   **Size**: refers to the size of the model, it's important to know that the name of the size is also the name of the model we will need to call to load it later on (for English models only it's the model name + .en)\n",
        "\n",
        "*   **Parameters**: these, in sum, refer to the computational efficiency and size of the model you are using. For more details see refs below (or to have a play with calcluating parameter sizes for models, see here: https://transformerparameters-calculator.streamlit.app/)\n",
        "\n",
        "*   **Required VRAM**: The amount of memory space you will need available to run the model.\n",
        "\n",
        "*   **Relative speed**: The speed it will take to run the model in comparison to the slowest model (large).\n",
        "\n",
        "N.B: The columns \"English-only model\" and \"Multilingual model\" are fairly self-explanatory, but essentially refer to whether the model is only for english data or will work on multilingual data as well."
      ],
      "metadata": {
        "id": "PFAdgP_8A89M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('/content/Screenshot 2024-10-11 at 12.29.29 PM.png')\n",
        "display(img)"
      ],
      "metadata": {
        "id": "s25ixgTJH1CX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"turbo\")"
      ],
      "metadata": {
        "id": "hXJQeQX2Bugl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cd2419-11b2-42fe-b164-b9fe9d3f5eb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.51G/1.51G [00:21<00:00, 74.7MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**:\n",
        "\n",
        "Transcribe the data!\n",
        "\n",
        "To do is very simple, we call the function 'transcribe' from the whisper library.\n",
        "\n",
        "The function takes a string containing the file path to your audio file. We will also use the following arguments:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Language**: this tells whisper what language you want to transcribe from. In this case we will tell it we are trancribing from English ('en'). (See list of ISO codes in reference materials).\n",
        "\n",
        "2.   **Verbose**: setting verbose to True means that whisper will show us it's output as it's transcribing the data (with its log of warnings, time taken, and any other relevant stuff).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PySPwo9y-4kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: whipser looks for a string containing the filepath, not a filepath (i.e. Path(\"path/to/file.wav\")).\n",
        "\n",
        "audiofile = \"/content/28158-15-3228993-task-lj3f-10598718-animalsenglish-3-2.wav\"\n",
        "\n",
        "result = model.transcribe(audiofile, language = \"en\", verbose = True)\n",
        "result[\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "Und6uMns3JLR",
        "outputId": "d94ccaf0-ceb1-4eb8-ee4c-7ebbef521d07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:26.800]  Dogs, cats, horses, pigs, wolves, badgers, foxes, weasels, stoats, ferrets, squirrels, sheep, cows, pigs, horses, goats, rabbits, whales, elephants, giraffes, tigers, lions.\n",
            "[00:26.800 --> 00:31.160]  Dolphins.\n",
            "[00:33.240 --> 00:36.600]  Gazelles, deer.\n",
            "[00:42.600 --> 00:47.280]  Rhinoceroses, hippopotamuses.\n",
            "[00:49.400 --> 00:52.440]  Hedgehogs.\n",
            "[00:56.800 --> 01:00.080]  Wild Caps.\n",
            "[01:00.120 --> 01:01.160] ulei Valudjaros.\n",
            "[01:03.200 --> 01:05.240]  Hjistum við tóraum að Sól og hverfolaðan í þetta Nesna al người undernoð.\n",
            "[01:05.240 --> 01:18.680]  Flnin er tórið gjordfið alveginarstörsk\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Dogs, cats, horses, pigs, wolves, badgers, foxes, weasels, stoats, ferrets, squirrels, sheep, cows, pigs, horses, goats, rabbits, whales, elephants, giraffes, tigers, lions. Dolphins. Gazelles, deer. Rhinoceroses, hippopotamuses. Hedgehogs. Wild Caps.ulei Valudjaros. Hjistum við tóraum að Sól og hverfolaðan í þetta Nesna al người undernoð. Flnin er tórið gjordfið alveginarstörsk<|ko|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional**:\n",
        "\n",
        "Because Whisper used GPT predictive capacity to transcribe audio, you can also give it a 'helping hand' by prompting it on the content of the audio.\n",
        "\n",
        "The example audio I am using in this notebook is from verbal fluency taks I collected during my PhD thesis. Therefore, I am going to prompt Whisper that the audio in this file contains content from a verbal fluency task. I will also give it a further description of the task itself.\n",
        "\n",
        "*Let's see if it's any better!*"
      ],
      "metadata": {
        "id": "qvBgn6qzAy5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    f\"This is a computer recorded audio file containing a verbal fluency task.\"\n",
        "    f\"The speaker is a research subject and is naming all the animals\"\n",
        "    f\"they can think of in under sixty seconds.\")\n",
        "\n",
        "result = model.transcribe(audiofile, language='en', verbose=True, initial_prompt=prompt)\n",
        "result[\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "eLzCABMG9Krw",
        "outputId": "f407dece-b6cb-48ef-a277-dcdae9a79a8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:26.820]  Dogs, cats, horses, pigs, wolves, badgers, foxes, weasels, stoats, ferrets, squirrels, sheep, cows, pigs, horses, goats, rabbits, whales, elephants, giraffes, tigers, lions.\n",
            "[00:26.820 --> 00:31.140]  Dolphins.\n",
            "[00:33.660 --> 00:36.580]  Gazelles, deer.\n",
            "[00:43.260 --> 00:47.280]  Rhinoceroses, hippopotamuses.\n",
            "[00:50.280 --> 00:52.440]  Hedgehogs.\n",
            "[00:56.820 --> 01:00.020]  Wildc elevate bancor.\n",
            "[01:00.020 --> 01:15.940]  Many祖 props toücken their homeland in Michigan, they were drawn by gold militant, Georgios, Geovits, amputin andosis in the action of the population and distribution surg iconic patent in the colduna funds.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Dogs, cats, horses, pigs, wolves, badgers, foxes, weasels, stoats, ferrets, squirrels, sheep, cows, pigs, horses, goats, rabbits, whales, elephants, giraffes, tigers, lions. Dolphins. Gazelles, deer. Rhinoceroses, hippopotamuses. Hedgehogs. Wildc elevate bancor. Many祖 props toücken their homeland in Michigan, they were drawn by gold militant, Georgios, Geovits, amputin andosis in the action of the population and distribution surg iconic patent in the colduna funds.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "\n",
        "Whisper tutorial: https://christophergs.com/blog/ai-podcast-transcription-whisper\n",
        "\n",
        "Whisper landing page: https://github.com/openai/whisper\n",
        "\n",
        "Regarding parameters: https://medium.com/@geosar/understanding-parameter-calculation-in-transformer-based-models-simplified-e8c7f4e059d8\n",
        "\n",
        "Language ISO codes: https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes\n",
        "\n",
        "Some fun resources on YouTube: https://www.youtube.com/watch?v=wjZofJX0v4M&t=776s"
      ],
      "metadata": {
        "id": "fLUVfrINOKuI"
      }
    }
  ]
}